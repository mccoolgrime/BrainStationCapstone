# Hiking with iNaturalist: a BrainStation Capstone
<h1 align="center">
<img src="./coralroot.png" width="300">
</h1><br>

## Personal Connection
I've always been interested in plant identification and generally knew a few more things about the kinds of plants that might pop up in the sidewalk cracks in my neighborhood than the average person. When 2020 hit and quarantine shut down everywhere except my apartment and nature trails, I ended up spending a lot of time in my apartment and on nature trails. A good google search might help me learn the name of one new plant each week, but when a friend introduced me to iNaturalist, my knowledge base took off. The AI helped suggest a solid genus name for the plants I was taking pictures of and then other iNaturalist users would come along to help refine identifications at the species level. I spent an hour each day taking pictures along a hike, another 20 minutes uploading them and guessing their identity that night and another hour looking through the observations other users posted for my area during that same day--agreeing or disagreeing with their identifications if I knew enough about the plant to feel sure of myself. I continue to learn so much from being a part of the iNaturalist community and the wealth of information iNaturalist provides. Now that I am pursuing a degree in Data Science through BrainStation's bootcamp, I am thrilled to have new skills to apply to an old love.

## Context: But officially...what is iNaturalist? 
iNaturalist is a [“crowdsourced species identification system and an organism occurrence recording tool”](https://www.inaturalist.org/pages/about). It allows individual users to upload geolocated and timestamped observations of the natural living world (usually as pictures), allows the user to classify the observation up to the species level (with suggested help from an AI tool), and allows other users to verify or disagree with the user's classification.

## New Question:
__Can a model learn to “see” an iNaturalist user’s hikes?__
## Original Question:
What can machine learning tell us about the best practices for obscuring the geolocation of at-risk plants and the geolocation or other information of those observations made near it in time by the same user?

## Impact
### For New Question
1) Individual User Engagement--
*Example: iNaturalist could include a slidedeck or interactive map of a user’s “longest hike” in the personal “Year in Review” materials* 

2) Regional User Engagement/Expansion--
*Example: iNaturalist could partner with a city, identify common walking trails from their data and create a “scavenger hunt” of observations found on the trails*

3) Scientific --
*Example: identifying the number of “hikes” a user takes in relationship to the observations they contribute that aren’t clustered into hikes could be used as a potential measure for user behavior*

### For Original Question
Data on the poaching of plants is difficult to obtain although [the United Nations Conference on Trade and Development speculates a rise in poaching by looking at discrepancies between declared exports and imports](https://unctad.org/press-material/illegal-trade-accelerates-wild-plant-extinctions-more-transparency-needed). Additionally, there is not currently data accessible that tackles the questions: “Does iNaturalist make endangered species more or less vulnerable? And if so, in what ways and by how much?” (Although this would be an interesting place for research). However, as a nonprofit organization with a goal of conservation, iNaturalist recognizes the need to protect at-risk plants from poachers and acknowledges this particular problem of interpolation: [“We are working on structural changes to make interpolating locations more difficult, but in the meantime we recommend not posting any open observations nearby in space and time to where your obscured observation was posted”](https://www.inaturalist.org/pages/help#geoprivacy). So my project has the potential to offer helpful insights and guidance toward this goal that iNaturalist has already identified but for which iNaturalist hasn’t yet produced a solution. 

### Ethical Considerations
Because we live in a world where both humans and endangered species can be stalked with harmful intent, it should be noted as well that my project could have potentially harmful impact in the wrong hands. This means that I am striking an ethical balance here, with the assumption that those who will be looking at this project will be peers and educators in data science or potential employers who would not be interested or willing to share this information beyond that. For that reason, *I am asking that this project not be shared broadly but on a case-by-case basis, keeping the protection of humans and endangered species as an important value when choosing with whom to share, when and why.*

## Methodology
1) __Data Collection__ \
   a) *General*: iNaturalist has [an export tool](https://www.inaturalist.org/observations/export) for its data that works by creating queries of 200,000 or fewer observations at a single time. To access the tool, one needs an iNaturalist account which is free to create. While the technical limit is 200,000 or fewer observations, I have found that making queries near that threshold usually fail while queries of 100,000 or fewer entries usually succeed, although it should be noted that queries of more than 2000 - 3000 observations can still take quite a long time to load. The export website states that large requests slow down their infrastructure, making it harder to release new changes and suggest other sources if they meet the needs. Since my questions are specific to how iNaturalist uses data, it was necessary to use the iNaturalist database, however other readers might consider the alternatives offered on the export page depending on their own needs to keep the load lighter on iNaturalist's infrastructure. \
   \
   b) *Preliminary Exploration*: My first exploratory sets involved downloading two sets of top-posting users "gregtasney" and "evanaturalist" to get a sense of the columns of data, potential methodology and cleaning concerns. I have three Jupyter notebooks that work with these two datasets. The "prelim_exploration" notebooks are kept in the github mostly as documentation and I tried my best to summarize and include any findings from those in the Spicata notebooks where the bulk of my analysis is applied. The third notebook was an initial mock-up of a potential interpolation modeling process and analysis and the findings there are applied in the Spicata_EDA notebook. \
   \
   c) *ungberg and annegp datasets*: I wanted to ensure I had a sense of how a vulnerable species marked for conservation protection might appear in both iNaturalist search engines and datasets, so I focused on the Crested Coralroot Orchid, whose location I actually knew. I used the iNaturalist search engine to see how the data was displayed on their own page through a browser if I searched for Bletia spicata in my area: 6 observers with 8 observations from July 2016 to July 2022. The locations were obscured to a distance of up to about 27 km away and the times given were granularized to the month and year rather than date and time. I chose one of the 8 observations--ungberg in July 2020--and downloaded all of that user's data from that month to see what the data looked like on the backend. I did not explore via a Jupyter notebook, but in looking through the csv, it does not seem as though data requested through the export obscures the timestamps and even if it was obscuring the "time_observed_at" category, the id numbers for each observation seem to created with respect to the "time_created_by" function, which gives a potential category for ordering the data by proximity in time regardless unless the user intentionally uploads the observation at a much later time (which some users do for vulnerable plants). I later confirmed this with annegp's dataset, which I then used for illustation purposes in the Sprint 1 Capstone Presentation.\
   \
   d) *Spicata dataset*: (See Spicata_Clean_and_Prep and Spicata_Clean_and_Prep_MoreData notebook for more details). If someone is trying to find a vulnerable species, they would first start with a search of the species itself and then narrow down the set of users who have posted about those vulnerable plants--users whose personal dataset has "clues" as to where the location of the vulnerable plant is, even if the plant's location is publicly obscured. I can't train and test on an actual vulnerable species because I don't have access to location data that can validate how accurate my models' location predictions are. But I can choose a non-vulnerable target species that shares some characteristics with vulnerable species. I thought of Lobelia inflata because I'm fond of it but in this area it is more rare than other native species. When I ran it through the iNaturalist search engine, on a national level, it had 11,500+ observations. Comparatively, vulnerable species like Yellow Lady's Slipper or Spring Ladies' Tresses only had 4,000 - 5,000 U.S. observations. However, inflata's cousin--Lobelia spicata--only had 4,500 U.S. observations, but was not listed as needing conservation protection, which meant its taxon geoprivacy settings would not obscure the location data, making it a comparable species to study while still providing validation data. (I want to note here that I did not consciously know that the Crested Coralroot Orchid's species level name was the same as the target species I eventually landed on, but I am too far down the road of analysis to change it now and leave it here as evidence of unintentional human bias, which is, at least here, bias towards things I adore--certainly not the worst kind of bias we encounter in the world of data science. I hope my other motivations for choosing Lobelia spicata will protect the analysis from too much skew due to this unconscious affinity for the name spicata, but would also welcome and accept any study proving that I am wrong about this. I will be testing my findings on other target species to make sure they are replicable.) In the Spicata_Clean_and_Prep notebook, I downloaded the set of all Lobelia spicata observations in the U.S., identified the set of unique users whose spicata observations had locations listed at a research-grade level of positional accuracy and then used random shuffle functions on that set of users to choose sets of users to query from the iNaturalist database in groups of 10 until I reached at least 100,000 observations AND at least 100 Lobelia spicata observations with public positional accuracy of 30m or less. This took 70 users to accomplish. Once that set was constructed, I cleaned the data and downloaded the csv for future use for EDA and modeling. (See "Description of Dataset" below.
   
2) __EDA__ <br> (See Spicata_EDA and Spicata_EDA_MoreData for more details) \
   General takeaways: <br>
   a) For this particular kind of target, the data behaves hyperlocally. The closer in time a data point *made by the same user* is to the observation with a missing location, the more that data point is likely to help predict the missing location. The further in time away, the more likely it is to fail at predicting or even lead the predictions away from the missing location. Models that help identify clusters of data in time and space will be much more successful at predicting locations than models that don't consider this.<br>
   b) There are three categories of data that can be used to perceive a cluster in time: i) "time_observed_at" (logically)  <br> ii) "created_at" because users most often create the observations soon after making them (and probably in the same sequential order, but I did not directly study this) <br> iii) "id" because the unique observation id is assigned consecutively based on when observations are created.
   
3) __Initial Exploration of Linear Interpolation__ <br>(see Spicata_EDA and Spicata_EDA_MoreData for more details) <br>
   For each observation in the training data: <br>a) identify the user for that spicata observation <br>b) locate up to 100 observations by that user--50 directly before, 50 directly after--based off the time_observed_at column <br>c) use linear interpolation as mapped out in the mock-up to predict the location and record its accuracy and also answer the question: how many observation locations would need to be obscured to protect this location at a given standard? 
   The answer for this set: over half of the time 2 is the best. Hyperlocality--just the datapoints on either side of the target when ordered by time. Using just these 2 points and linear interpolation, I was able to predict a target species' location to within 100 (approximate) human steps 55% of the time. 
   
4) __Recommendations for Original Question__ <br>(see Sprint1_Presentation and Sprint2_Presentation)<br>
   Based off the initial exploration, it became clear that basic data analytics and light data science was all that was required to make solid recommendations which are as follows:<br>
   a) Obscure the “observed_at_time” online and in the dataset

   b) Obscure the “created_at” time online and in the dataset

   c) Research and implement a way to assign id numbers to observations that are not based on when the observation is created

5) __Pivot to New Question__ <br>(see the notebook: Applying ML to Find User Hikes - Case Study) <br>
   I pulled a user at random from the dataset I created and applied a heuristic approach for finding clusters that approximate hikes and then compared those results to results created by various DBSCAN applications. For DBSCAN, I tested different feature selection as well as hyperparameters and compared silhouette scores. 

6) __Final Modeling__<br>(see the notebook: Applying ML to Find User Hikes - Comparing Users) <br>
   I applied the heuristic model and DBSCAN models to 225 users, creating a metadataset of silhouette scores and other metrics. I performed EDA on the metadataset to determine the best model. I applied the chosen model and used it to visualize user hikes as well as begin to create metrics for describing user behavior.

## Description of Dataset 
  This data dictionary describes the "spicata_clean" data on which I performed my EDA and initial modeling. For construction of this dataset, see *Spicata dataset* above. Below are the columns of data I found relevant for EDA and modeling for the first round of initial modeling. Future rounds may pull in other columns available via iNaturalist.
  - __id__: int of unique id of species, appears to be assigned at upload, generated consecutively in time and therefore directly correlated to the "created_at" column
  - __observed_on_string__: string related to the time_observed_at stamp--included for cross referencing
  - __observed_on__: string related to the time_observed_at stamp--included for cross referencing 
  - __time_observed_at__: timestamp of when the observation was made given in UTC time *needs to be converted from object to timestamp*
  - __time_zone__: string stating time zone observation was taken in, needs cleaning but is present in case the non ML model is chosen, to get a more precise understanding of the "day" from the observer perspective
  - __user_id__: int of unique numerical id of user, appears to be assigned when account is created, generated consecutively in time and therefore small numbers point to early adopters
  - __created_at__: timestamp of when the observation was uploaded to iNaturalist given in UTC time *needs to be converted from object to timestamp*
  - __quality_grade__: string, there is an algorithm that determines if an observation has reached "research" level but roughly at least two users agree on the species level AND over 2/3 of the user ids for this observation are in agreement; "needs_id" if the observation contains all the necessary information required to make "research grade" but is missing the needed ratio of user identifications; "casual" means it is missing important data components and cannot be used for "research grade". *This column can be hot-encoded*
  - __url__: string of the url that will take a user to the observation on iNaturalist
  - __image_url__: string of the url to the image for an observation, if included, null values allowed because the reference is meaningful if it exists
  - __sound_url__: string of the url to the sound file for an observation, if included, null values allowed because the reference is meaningful if it exists
  - __num_identification_agreements__: int, the number of additional users who agree with the first user who landed at the "species_guess"
  - __num_identification_disagreements__: int, the number of users who disagree with the "species_guess" (doesn't include "withdrawn" identifications for users who originally disagreed but withdrew earlier contradicting guesses)
  - __captive_cultivated__: boolean, [further information here](https://www.inaturalist.org/pages/help#:~:text=Quality%20Assessment%20section.-,What%20does%20captive%20%2F%20cultivated%20mean%3F,to%20be%20then%20and%20there.)
  - __latitude__: float, latitude of the observation--it is my deduction that this is associated with the public_positional_accuracy
  - __longitude__: float, see above
  - __positional_accuracy__: float, distance in meters, it is my deduction that this is associated with private lat/lon coordinates that are not available in this dataset or publicly--the accuracy of the posting, before geoprivacy is applied. [See here for more information](https://www.inaturalist.org/posts/2035-observation-location-accuracy)
  - __public_postitional_accuracy__: float, distance in meters--it is my deduction that this is the accuracy of the given "latitude" and "longitude" available in this dataset. See "positional_accuracy" for more information
  - __coordinates_obscured__: boolean--requires more exploration, but likely an umbrella category for both geoprivacy and taxon_geoprivacy
  - __species_guess__: string, not actually at the taxon species level: the most specific taxon "guess" that is provided by the consensus/majority of "guesses" provided by user identifications--this is an if/then process: if all users are in agreement, this is the guess; if users disagree, then the guess with more than half the votes becomes the "guess" *double check this criteria*, if neither of these two criteria is satisfied, the "guess" defaults to the next higher taxon that would include all user guesses which can be as broad as "Life"
  - __scientific_name__: string, the scientific name of the most granular taxon level at which the observation was identified without contradiction
  - __common_name__: string, a "common" translation of the scientific name
  - __taxon_kingdom_name__: string, based on the "species guess", the kingdom of the observation if identified at this level *for later hot-encoding, only Animal, Plant and Fungi are recommended*
  - __taxon_genus_name__: string, based on the "species guess",the genus of the observation if identified at this level 
  - __taxon_species_name__: string, the species of the observation, based on the "species guess" if identified at this level
  - __geoprivacy_obscured__: boolean--1 means observation was marked "obscured" for geoprivacy, 0 means it was null in original dataset, it is my deduction that this is in reference to a user's personal choice to make an observation obscured or not 
  - __taxon_geoprivacy_obscured__: boolean, 1 means observation was marked "obscured", 0 was "open" or null in the original dataset, it is my deduction that this in reference to conservation measures taken according to the taxon if a taxon is marked as protected
  - __minute_diff__: float, number of minutes between previous observation and this one when observations sorted by user and then time_observed_at. Initial observations for each user assigned the value -0.001 (I created this column--see Spicata_Clean_and_Prep for more details)
  - __km_diff__: float, number of km between previous observation and this one when observations sorted by user and then time_observed_at. Initial observations for each user assigned the value -0.001. (I created this column--see Spicata_Clean_and_Prep for more details)
   

